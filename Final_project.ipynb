{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from model.yolo_model import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhenya\\Anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\keras\\engine\\saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "def process_image(img):\n",
    "    \"\"\"Resize, reduce and expand image.\n",
    "\n",
    "    # Argument:\n",
    "        img: original image.\n",
    "\n",
    "    # Returns\n",
    "        image: ndarray(64, 64, 3), processed image.\n",
    "    \"\"\"\n",
    "    image = cv2.resize(img, (416, 416), interpolation=cv2.INTER_CUBIC)\n",
    "    image = np.array(image, dtype='float32')\n",
    "    image /= 255.\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "def get_classes(file):\n",
    "    \"\"\"Get classes name.\n",
    "\n",
    "    # Argument:\n",
    "        file: classes name for database.\n",
    "\n",
    "    # Returns\n",
    "        class_names: List, classes name.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def draw(image, boxes, scores, classes, all_classes):\n",
    "    \"\"\"Draw the boxes on the image.\n",
    "\n",
    "    # Argument:\n",
    "        image: original image.\n",
    "        boxes: ndarray, boxes of objects.\n",
    "        classes: ndarray, classes of objects.\n",
    "        scores: ndarray, scores of objects.\n",
    "        all_classes: all classes name.\n",
    "    \"\"\"\n",
    "    for box, score, cl in zip(boxes, scores, classes):\n",
    "        x, y, w, h = box\n",
    "\n",
    "        top = max(0, np.floor(x + 0.5).astype(int))\n",
    "        left = max(0, np.floor(y + 0.5).astype(int))\n",
    "        right = min(image.shape[1], np.floor(x + w + 0.5).astype(int))\n",
    "        bottom = min(image.shape[0], np.floor(y + h + 0.5).astype(int))\n",
    "\n",
    "        cv2.rectangle(image, (top, left), (right, bottom), (0, 255, 0), 2)\n",
    "        cv2.putText(image, '{0} {1:.2f}'.format(all_classes[cl], score),\n",
    "                    (top, left - 6),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.6, (0, 0, 255), 1,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "        print('class: {0}, score: {1:.2f}'.format(all_classes[cl], score))\n",
    "        print('box coordinate x,y,w,h: {0}'.format(box))\n",
    "    print()\n",
    "    \n",
    "def detect_image(image, yolo, all_classes):\n",
    "    \"\"\"Use yolo v3 to detect images.\n",
    "\n",
    "    # Argument:\n",
    "        image: original image.\n",
    "        yolo: YOLO, yolo model.\n",
    "        all_classes: all classes name.\n",
    "\n",
    "    # Returns:\n",
    "        image: processed image.\n",
    "    \"\"\"\n",
    "    pimage = process_image(image)\n",
    "\n",
    "    start = time.time()\n",
    "    boxes, classes, scores = yolo.predict(pimage, image.shape)\n",
    "    end = time.time()\n",
    "\n",
    "    print('time: {0:.2f}s'.format(end - start))\n",
    "\n",
    "    if boxes is not None:\n",
    "        draw(image, boxes, scores, classes, all_classes)\n",
    "\n",
    "    return image\n",
    "\n",
    "yolo = YOLO(0.6, 0.5)\n",
    "file = 'data/coco_classes.txt'\n",
    "all_classes = get_classes(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating face recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rotation_range=30,\n",
    "                               width_shift_range=0.1,\n",
    "                               height_shift_range=0.1,\n",
    "                               rescale=1/255,\n",
    "                               shear_range=0.2,\n",
    "                               zoom_range=0.2,\n",
    "                               horizontal_flip=True,\n",
    "                               fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "image_gen_test_face = image_gen.flow_from_directory('../Final_Project/face_images/test',\n",
    "                                               target_size=(150, 150),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_train_face = image_gen.flow_from_directory('../Final_Project/face_images/train',\n",
    "                                               target_size=(150, 150),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_face = Sequential()\n",
    "\n",
    "model_face.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(150,150,3),activation='relu',))\n",
    "model_face.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model_face.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(150,150,3),activation='relu',))\n",
    "model_face.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model_face.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(150,150,3),activation='relu',))\n",
    "model_face.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model_face.add(Flatten())\n",
    "\n",
    "model_face.add(Dense(128))\n",
    "model_face.add(Activation('relu'))\n",
    "\n",
    "model_face.add(Dropout(0.5))\n",
    "\n",
    "model_face.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model_face.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9248)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1183872   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,203,780\n",
      "Trainable params: 1,203,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_face.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2409 - acc: 0.8931 - val_loss: 0.1500 - val_acc: 0.9500\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 52s 522ms/step - loss: 0.0505 - acc: 0.9862 - val_loss: 0.0433 - val_acc: 0.9500\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 0.0116 - acc: 0.9950 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 53s 528ms/step - loss: 0.1374 - acc: 0.9625 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 49s 490ms/step - loss: 0.0278 - acc: 0.9919 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 53s 529ms/step - loss: 0.0251 - acc: 0.9919 - val_loss: 0.0167 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 53s 531ms/step - loss: 0.0125 - acc: 0.9969 - val_loss: 0.0401 - val_acc: 0.9750\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.0180 - acc: 0.9975 - val_loss: 9.5900e-04 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 49s 493ms/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.3217e-06 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0089 - acc: 0.9969 - val_loss: 1.5798e-05 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "result = model_face.fit_generator(image_gen_train_face, epochs = 10,\n",
    "                            steps_per_epoch = 100,\n",
    "                            validation_data = image_gen_test_face,\n",
    "                            validation_steps = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.893125,\n",
       " 0.98625,\n",
       " 0.995,\n",
       " 0.9625,\n",
       " 0.991875,\n",
       " 0.991875,\n",
       " 0.996875,\n",
       " 0.9975,\n",
       " 0.999375,\n",
       " 0.996875]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_face.save('face_recognition.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use face recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_face = load_model('face_recognition.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abeshev K': 0, 'Adelina': 1, 'Eduard': 2, 'Evgenii': 3}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_gen_train_face.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = {0: 'Kuanysh',\n",
    "         1: 'Adelina',\n",
    "         2: 'Eduard',\n",
    "         3: 'Evgenii'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is Eduard with probability 1.0\n",
      "It is Eduard with probability 1.0\n",
      "It is Eduard with probability 1.0\n",
      "It is Eduard with probability 1.0\n",
      "It is Eduard with probability 1.0\n",
      "It is Eduard with probability 1.0\n",
      "It is Eduard with probability 1.0\n",
      "It is Eduard with probability 1.0\n",
      "It is Eduard with probability 1.0\n",
      "It is Eduard with probability 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    img = '../Final_Project/face_images/test/Eduard/' + str(i) + '.jpg'\n",
    "    img = image.load_img(img,target_size=(150,150))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    img = img / 255\n",
    "\n",
    "    pred_array = model_face.predict(img)\n",
    "    print('It is', faces[np.argmax(pred_array)], 'with probability', pred_array.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "    face_img = img.copy()\n",
    "    face_rects = face_cascade.detectMultiScale(face_img) \n",
    "    \n",
    "    for (x,y,w,h) in face_rects:\n",
    "        face_img = face_img[y:y+h,x:x+w]\n",
    "        cv2.rectangle(face_img, (x,y), (x+w,y+h), (255,255,255), 10) \n",
    "        \n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_face(image):\n",
    "    image = np.array(image, dtype='float32')\n",
    "    image /= 255\n",
    "    start = time.time()\n",
    "    pred_array = model_face.predict(image)\n",
    "    end = time.time()\n",
    "    result = faces[np.argmax(pred_array)]\n",
    "    score = float(\"%0.2f\" % (max(pred_array[0])))\n",
    "    print('time: {0:.2f}s'.format(end - start))\n",
    "    print(f'pred_array: {pred_array}')\n",
    "    print(f'class: {result}, score: {score}')\n",
    "    print()\n",
    "    return result, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating images for gesture detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_background(frame):\n",
    "    fgmask = bgModel.apply(frame, learningRate=0)\n",
    "    kernel = np.ones((3, 3), np.uint8) # ядро\n",
    "    fgmask = cv2.erode(fgmask, kernel, iterations=1) # размытие изображения\n",
    "    res = cv2.bitwise_and(frame, frame, mask=fgmask) # вычисляет побитовое пересечение двух массивов\n",
    "    gray = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0) # размытие\n",
    "    ret, thresh = cv2.threshold(blur, 10, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) #вычисляет границу\n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap_region_x_begin = 0.5\n",
    "cap_region_y_end = 0.5\n",
    "num = 1\n",
    "isBgCaptured = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    h = frame.shape[0]\n",
    "    w = frame.shape[1]\n",
    "    frame = cv2.flip(frame, 1) \n",
    "    frame_copy = frame[61:int(0.5*h)+1, int(0.5*w):w]\n",
    "    cv2.rectangle(frame, (int(cap_region_x_begin * frame.shape[1]), 60),(frame.shape[1], int(cap_region_y_end * frame.shape[0])), (0, 255, 0), 2)\n",
    "    \n",
    "    if isBgCaptured == 1:\n",
    "        frame_copy = remove_background(frame_copy)\n",
    "        frame[61:int(0.5*h)+1, int(0.5*w):w,0] = frame_copy\n",
    "        frame[61:int(0.5*h)+1, int(0.5*w):w,1] = frame_copy\n",
    "        frame[61:int(0.5*h)+1, int(0.5*w):w,2] = frame_copy\n",
    "        \n",
    "    cv2.imshow(\"original\", frame)\n",
    "    \n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('b'):\n",
    "        bgModel = cv2.createBackgroundSubtractorMOG2(0, 50)\n",
    "        isBgCaptured = 1\n",
    "    elif k == 32:\n",
    "        direc = \"../Final_Project/gest_images/\" + str(num) + '.jpg'\n",
    "        cv2.imwrite(direc, frame_copy)\n",
    "        cv2.imshow(\"save image\", frame_copy)\n",
    "        num += 1\n",
    "        \n",
    "cap.release()        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating gesture detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rotation_range=30,\n",
    "                               width_shift_range=0.1,\n",
    "                               height_shift_range=0.1,\n",
    "                               rescale=1/255,\n",
    "                               shear_range=0.2,\n",
    "                               zoom_range=0.2,\n",
    "                               horizontal_flip=True,\n",
    "                               fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "image_gen_test = image_gen.flow_from_directory('../Final_Project/gest_images/test',\n",
    "                                               target_size=(150, 150),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_train = image_gen.flow_from_directory('../Final_Project/gest_images/train',\n",
    "                                               target_size=(150, 150),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(150,150,3),activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(150,150,3),activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(150,150,3),activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9248)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1183872   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,203,909\n",
      "Trainable params: 1,203,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 1.4461 - acc: 0.3825 - val_loss: 1.1944 - val_acc: 0.5800\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 1.1191 - acc: 0.5544 - val_loss: 0.8725 - val_acc: 0.7200\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 0.8470 - acc: 0.6738 - val_loss: 0.6153 - val_acc: 0.7600\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.7137 - acc: 0.7287 - val_loss: 0.6056 - val_acc: 0.7800\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.5992 - acc: 0.7656 - val_loss: 0.5728 - val_acc: 0.7600\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.5443 - acc: 0.8000 - val_loss: 0.4284 - val_acc: 0.7800\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 13s 129ms/step - loss: 0.4664 - acc: 0.8281 - val_loss: 0.2881 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.4651 - acc: 0.8262 - val_loss: 0.3992 - val_acc: 0.8400\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 13s 129ms/step - loss: 0.4093 - acc: 0.8494 - val_loss: 0.2971 - val_acc: 0.9200\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.3517 - acc: 0.8706 - val_loss: 0.2346 - val_acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "result = model.fit_generator(image_gen_train, epochs = 10,\n",
    "                            steps_per_epoch = 100,\n",
    "                            validation_data = image_gen_test,\n",
    "                            validation_steps = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38406735751295334,\n",
       " 0.5559895833333334,\n",
       " 0.6764322916666666,\n",
       " 0.7252604166666666,\n",
       " 0.759765625,\n",
       " 0.8011658031088082,\n",
       " 0.8274739583333334,\n",
       " 0.8235677083333334,\n",
       " 0.8484455958549223,\n",
       " 0.8704427083333334]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gesture_recognition.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('gesture_recognition.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'five': 0, 'like': 1, 'okay': 2, 'peace': 3, 'rock': 4}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_gen_train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_names = {0: 'five',\n",
    "                 1: 'like',\n",
    "                 2: 'okay',\n",
    "                 3: 'peace',\n",
    "                 4: 'rock'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is rock with probability 0.9654696\n",
      "It is rock with probability 0.96290535\n",
      "It is rock with probability 0.98637295\n",
      "It is rock with probability 0.94601667\n",
      "It is rock with probability 0.7545623\n",
      "It is peace with probability 0.66241014\n",
      "It is rock with probability 0.5782857\n",
      "It is rock with probability 0.86349607\n",
      "It is rock with probability 0.73312294\n",
      "It is rock with probability 0.53770334\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    img = '../Final_Project/gest_images/test/rock/' + str(i) + '.jpg'\n",
    "    img = image.load_img(img,target_size=(150,150))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    img = img / 255\n",
    "\n",
    "    pred_array = model.predict(img)\n",
    "    print('It is', gesture_names[np.argmax(pred_array)], 'with probability', pred_array.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    image = np.array(image, dtype='float32')\n",
    "    image /= 255\n",
    "    start = time.time()\n",
    "    pred_array = model.predict(image)\n",
    "    end = time.time()\n",
    "    result = gesture_names[np.argmax(pred_array)]\n",
    "    score = float(\"%0.2f\" % (max(pred_array[0])))\n",
    "    print('time: {0:.2f}s'.format(end - start))\n",
    "    print(f'pred_array: {pred_array}')\n",
    "    print(f'class: {result}, score: {score}')\n",
    "    print()\n",
    "    return result, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.36s\n",
      "pred_array: [[6.63916653e-05 1.00941464e-01 6.44835532e-01 2.54156619e-01]]\n",
      "class: Eduard, score: 0.64\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[1.6125650e-04 2.4863054e-01 6.0382217e-01 1.4738601e-01]]\n",
      "class: Eduard, score: 0.6\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[6.5308479e-05 1.3240877e-01 6.5089619e-01 2.1662971e-01]]\n",
      "class: Eduard, score: 0.65\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[1.2297514e-04 2.9695520e-01 5.6289995e-01 1.4002186e-01]]\n",
      "class: Eduard, score: 0.56\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[0.00226959 0.32725343 0.62314737 0.04732965]]\n",
      "class: Eduard, score: 0.62\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[0.00189731 0.28763607 0.6603198  0.05014683]]\n",
      "class: Eduard, score: 0.66\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[0.00063829 0.40889904 0.55101675 0.03944592]]\n",
      "class: Eduard, score: 0.55\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[1.8172902e-04 2.3327929e-01 7.0885485e-01 5.7684142e-02]]\n",
      "class: Eduard, score: 0.71\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[1.4298796e-04 6.9481574e-02 8.8091338e-01 4.9462002e-02]]\n",
      "class: Eduard, score: 0.88\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[1.1987343e-04 4.0418744e-02 9.1408235e-01 4.5379009e-02]]\n",
      "class: Eduard, score: 0.91\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[0.00077115 0.5577402  0.40058818 0.04090053]]\n",
      "class: Adelina, score: 0.56\n",
      "\n",
      "time: 0.00s\n",
      "pred_array: [[0.00724665 0.42876375 0.4934211  0.07056846]]\n",
      "class: Eduard, score: 0.49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "num = 1\n",
    "isBgCaptured = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    h = frame.shape[0]\n",
    "    w = frame.shape[1]\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_copy = frame[60:int(0.5*h), int(0.5*w):w]\n",
    "    frame_copy2 = frame[240:420, int(0.5*w):w]\n",
    "    frame_copy3 = frame[60:420, 0:int(0.5*w)]\n",
    "    cv2.rectangle(frame, (int(0.5 * w), 60),(w, int(0.5 * h)), (0, 255, 0), 2)\n",
    "    cv2.rectangle(frame, (int(0.5 * w), 240),(w, 420), (0, 0, 255), 2)\n",
    "    cv2.rectangle(frame, (0, 60), (int(0.5 * w), 420), (255, 0, 0), 2)\n",
    "    \n",
    "    if isBgCaptured == 1:\n",
    "        frame_copy = remove_background(frame_copy)\n",
    "        frame[61:int(0.5*h)+1, int(0.5*w):w,0] = frame_copy\n",
    "        frame[61:int(0.5*h)+1, int(0.5*w):w,1] = frame_copy\n",
    "        frame[61:int(0.5*h)+1, int(0.5*w):w,2] = frame_copy\n",
    "    \n",
    "    cv2.imshow(\"original\", frame)\n",
    "    \n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('b'):\n",
    "        bgModel = cv2.createBackgroundSubtractorMOG2(0, 50) # function that recognizes moving objects and calculate background\n",
    "        isBgCaptured = 1\n",
    "    elif k == ord('g') and isBgCaptured == 1:\n",
    "        target = np.stack((frame_copy,) * 3, axis=-1)\n",
    "        target = cv2.resize(target, (150, 150))\n",
    "        target = target.reshape(1, 150, 150, 3)\n",
    "        prediction, score = predict_image(target)\n",
    "        res = cv2.putText(frame_copy, f\"{prediction} : {score}\", (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255, 255, 255))\n",
    "        frame_copy = cv2.add(frame_copy,res)\n",
    "        cv2.imshow('gest_detection', frame_copy)\n",
    "    elif k == ord('o'):\n",
    "        frame_copy2 = detect_image(frame_copy2, yolo, all_classes)\n",
    "        cv2.imshow(\"object_detection\", frame_copy2)\n",
    "    elif k == ord('f'):\n",
    "        target = cv2.resize(frame_copy3, (150,150))\n",
    "        target = image.img_to_array(target)\n",
    "        target = np.expand_dims(target,axis=0)\n",
    "        prediction, score = predict_face(target)\n",
    "        res = cv2.putText(frame_copy3, f\"{prediction} : {score}\", (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255, 255, 255))\n",
    "        frame_copy3 = cv2.add(frame_copy3,res)\n",
    "        cv2.imshow('face_detection', frame_copy3)\n",
    "        \n",
    "cap.release()        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
